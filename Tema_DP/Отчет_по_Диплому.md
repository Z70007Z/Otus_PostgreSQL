
# Диплом:
# Создание и тестирование высоконагруженного отказоустойчивого кластера PostgreSQL на базе Patroni.
## 1. Разворачиваю виртуальные машины на площадке Yandex Cloud:

Была создана три виртуальные машина с характеристиками:

1. ОС - Ubuntu 24.04 LTS
2. Выделено минимально количество ресурсов
    - CPU 2 
    - RAM 2 ГБ
    - Диск HDD 20 Гб
    - Прерываемая - Да

3. В Дефолтной сети создал подсетку:
    - Имя: otus-vm-db-pg-net-1
    - ip адресация: 10.10.8.0/24
    - Динамический публичный IP адрес

4. Имена:
   - pg-node-1
   - pg-node-2
   - pg-node-3

5. Для подключения по SSH воспользовался ранее сгенерированным ключом, во время создания VM указал публичный ключ.

  Для подключения использовал  ПО "Visual Studio Code", можно так же Powershell, PuTTy и другое ПО.

## -----------------------------------------------------------------------------------------
## Для удобства работы изменяю случайно выданные ip адреса на последовательно расположенные:

Выполнил работы через интерфейс командной строки Yandex Cloud (CLI) <br/>
Ранее на отдельной машине выполнил установку ПО и инициализацию подключения. <br/>
https://yandex.cloud/ru/docs/cli/quickstart#install

```bash

export YC_TOKEN=$(yc iam create-token)
export YC_CLOUD_ID=$(yc config get cloud-id)
export YC_FOLDER_ID=$(yc config get folder-id)

yc compute instance list

yc compute instance get epd4lnpe031j5gnv2d7d
yc compute instance update-network-interface \
  --id epd4lnpe031j5gnv2d7d \
  --ipv4-address 10.10.8.26 \
  --network-interface-index 0


И т.д. для каждой машины, где требуется изменить ip.

# Картинка с информацией о подготовленных серверах (Картинка ниже!):
```
![Yandex Cloud](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/YandexCloud.jpg "Yandex Cloud")

## -------------------------------------------------------------------
## 1. Установка etcd на трех хостах. ---------------------------------

```bash

sudo apt update
sudo apt install etcd-server -y
sudo systemctl stop etcd
sudo systemctl disable etcd

# Удаляем конфигурацию по умолчанию
sudo rm -rf /var/lib/etcd/default

```
## -----------------------------------------------------
## Настройка etcd.
```bash
# Сделать резервную копию существующего файла конфигурации
sudo mv /etc/default/etcd /etc/default/etcd-orig

# Для узла 1, и на узле один настройка конфигурации:
sudo nano /etc/default/etcd

ETCD_NAME=node1 
ETCD_DATA_DIR="/var/lib/etcd/node1" 
ETCD_LISTEN_PEER_URLS="http://10.10.8.25:2380" 
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379" 
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.10.8.25:2380" 
ETCD_INITIAL_CLUSTER="node1=http://10.10.8.25:2380,node2=http://10.10.8.26:2380,node3=http://10.10.8.27:2380" 
ETCD_INITIAL_CLUSTER_STATE="new" 
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster" 
ETCD_ADVERTISE_CLIENT_URLS="http://0.0.0.0:2379" 
ETCD_ENABLE_V2="true"

# Для узла 2, и на узле два настройка конфигурации:
sudo nano /etc/default/etcd

ETCD_NAME=node2 
ETCD_DATA_DIR="/var/lib/etcd/node2" 
ETCD_LISTEN_PEER_URLS="http://10.10.8.26:2380" 
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379" 
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.10.8.26:2380" 
ETCD_INITIAL_CLUSTER="node1=http://10.10.8.25:2380,node2=http://10.10.8.26:2380,node3=http://10.10.8.27:2380" 
ETCD_INITIAL_CLUSTER_STATE="new" 
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster" 
ETCD_ADVERTISE_CLIENT_URLS="http://0.0.0.0:2379" 
ETCD_ENABLE_V2="true"

# Для узла 3, и на узле три настройка конфигурации:
sudo nano /etc/default/etcd

ETCD_NAME=node3
ETCD_DATA_DIR="/var/lib/etcd/node3"
ETCD_LISTEN_PEER_URLS="http://10.10.8.27:2380"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.10.8.27:2380"
ETCD_INITIAL_CLUSTER="node1=http://10.10.8.25:2380,node2=http://10.10.8.26:2380,node3=http://10.10.8.27:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ENABLE_V2="true"

```

Малая автоматизация чтобы не страдать: <br>
Создаем bash скрипт настройки ноды (пример для node2)

```bash
# Открываем новый файл
nano myscript.sh

# Записываем в него скрипт что ниже:
#----------------------------------------------------------------------------
#--------------Node1---------------------------------------------------------
#!/bin/bash

sudo apt update
sudo apt install etcd-server -y
sudo apt install etcd-client -y
sudo systemctl stop etcd
sudo systemctl disable etcd


# Удаляем конфигурацию по умолчанию
sudo rm -rf /var/lib/etcd/default

# Сделать резервную копию существующего файла конфигурации
sudo mv /etc/default/etcd /etc/default/etcd-orig

# настройка конфигурации:
sudo touch /etc/default/etcd

echo 'ETCD_NAME=node1'  >> /etc/default/etcd
echo 'ETCD_DATA_DIR="/var/lib/etcd/node1"'  >> /etc/default/etcd
echo 'ETCD_LISTEN_PEER_URLS="http://10.10.8.25:2380"'  >> /etc/default/etcd
echo 'ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.10.8.25:2380"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_CLUSTER="node1=http://10.10.8.25:2380,node2=http://10.10.8.26:2380,node3=http://10.10.8.27:2380"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_CLUSTER_STATE="new"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"'  >> /etc/default/etcd
echo 'ETCD_ADVERTISE_CLIENT_URLS="http://0.0.0.0:2379"'  >> /etc/default/etcd
echo 'ETCD_ENABLE_V2="true"'  >> /etc/default/etcd

sudo systemctl restart etcd
sudo systemctl status etcd
#----------------------------------------------------------------------------
#--------------Node2---------------------------------------------------------
#!/bin/bash

sudo apt update
sudo apt install etcd-server -y
sudo apt install etcd-client -y
sudo systemctl stop etcd
sudo systemctl disable etcd


# Удаляем конфигурацию по умолчанию
sudo rm -rf /var/lib/etcd/default

# Сделать резервную копию существующего файла конфигурации
sudo mv /etc/default/etcd /etc/default/etcd-orig

# настройка конфигурации:
sudo touch /etc/default/etcd

echo 'ETCD_NAME=node2'  >> /etc/default/etcd
echo 'ETCD_DATA_DIR="/var/lib/etcd/node2"'  >> /etc/default/etcd
echo 'ETCD_LISTEN_PEER_URLS="http://10.10.8.26:2380"'  >> /etc/default/etcd
echo 'ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.10.8.26:2380"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_CLUSTER="node1=http://10.10.8.25:2380,node2=http://10.10.8.26:2380,node3=http://10.10.8.27:2380"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_CLUSTER_STATE="new"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"'  >> /etc/default/etcd
echo 'ETCD_ADVERTISE_CLIENT_URLS="http://0.0.0.0:2379"'  >> /etc/default/etcd
echo 'ETCD_ENABLE_V2="true"'  >> /etc/default/etcd

sudo systemctl restart etcd
sudo systemctl status etcd
#----------------------------------------------------------------------------
#--------------Node3---------------------------------------------------------
#!/bin/bash

sudo apt update
sudo apt install etcd-server -y
sudo apt install etcd-client -y
sudo systemctl stop etcd
sudo systemctl disable etcd


# Удаляем конфигурацию по умолчанию
sudo rm -rf /var/lib/etcd/default

# Сделать резервную копию существующего файла конфигурации
sudo mv /etc/default/etcd /etc/default/etcd-orig

# настройка конфигурации:
sudo touch /etc/default/etcd

echo 'ETCD_NAME=node3'  >> /etc/default/etcd
echo 'ETCD_DATA_DIR="/var/lib/etcd/node3"'  >> /etc/default/etcd
echo 'ETCD_LISTEN_PEER_URLS="http://10.10.8.27:2380"'  >> /etc/default/etcd
echo 'ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.10.8.27:2380"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_CLUSTER="node1=http://10.10.8.25:2380,node2=http://10.10.8.26:2380,node3=http://10.10.8.27:2380"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_CLUSTER_STATE="new"'  >> /etc/default/etcd
echo 'ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"'  >> /etc/default/etcd
echo 'ETCD_ADVERTISE_CLIENT_URLS="http://0.0.0.0:2379"'  >> /etc/default/etcd
echo 'ETCD_ENABLE_V2="true"'  >> /etc/default/etcd

sudo systemctl restart etcd
sudo systemctl status etcd
#-----------------------------------------------------------------------
#-----------------------------------------------------------------------

# объявляем файл myscript.sh как bash скрипт
chmod +x myscript.sh
# запускаем:
./myscript.sh
```

И так можно все ноды быстро ввести в работу.

## -----------------------------------------------------
## Тестирование кластер etcd

```bash

cd ~
nano .profile

export ETCDCTL_API="3"
export PATRONI_ETCD_URL="http://127.0.0.1:2379"
export PATRONI_SCOPE="pg_cluster"
node1=10.10.8.25
node2=10.10.8.26
node3=10.10.8.27
ENDPOINTS=$node1:2379,$node2:2379,$node3:2379

sudo systemctl stop etcd
sudo systemctl start etcd
sudo systemctl restart etcd
sudo systemctl enable etcd
sudo systemctl status etcd

# Возникли сложности с именование узлов в конфиг. файлах.
# Хотел как имя на ПК pg-node-1, pg-node-2 ...
# Но получил ошибку, пришлось для первого раза остановиться на node1,node2,node3

```
## -----------------------------------------------------

```bash

source ~/.profile
etcdctl endpoint status --write-out=table --endpoints=$ENDPOINTS

# Картинка с выполненными командами ранее с информацией о кластере (Картинка ниже!):
``` 
![Информация о статусе кластера](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/info_status_cluster.jpg "Информация о статусе кластера")

```bash


sudo ss -an4p |grep 2379
sudo systemctl stop etcd

```
### Проводя эксперименты над узлом 2 потом было сложно вернуть его в работу
### ,т.к. были постоянные проблемы при проверки статуса
### попробовал разные варианты, бросил это дело и переустановил ПК
### Но ничего не изменилось.
### А вот остановка всех узлов и повторный из запуск позволил запустить службу на узле 2 и ошибки ушли.



## -----------------------------------------------------
## Установка PostgreSQL and psql client на все три ноды.
```bash

# Устанавливаем последнюю версию postgresql с некоторыми доп. ПО
sudo apt update && sudo apt upgrade -y 
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list' 
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - 
sudo apt-get update 
sudo apt-get -y install postgresql postgresql-client
sudo apt install unzip 
sudo apt -y install mc

# Проверяем статус
sudo systemctl status postgresql

# Останавливаем postgresql:
sudo systemctl stop postgresql
sudo systemctl disable postgresql

# Создаем символическую ссылку (/usr/sbin/) для Patroni на каждом узле
sudo ln -s /usr/lib/postgresql/16/bin/* /usr/sbin/
# Проверка работы ссылки:
ls -l /usr/sbin/

# Удаляем кластер postgresql
sudo su postgres
rm -rf /var/lib/postgresql/17/main/*
exit

```
## ----------------------------------------------------- 
## Установка и настройка Patroni на всех узлах:

```bash

# Установка нужных пакетов:
sudo apt -y install python3 python3-pip python3-dev libpq-dev
sudo pip3 install launchpadlib
sudo pip3 install --upgrade setuptools
sudo pip3 install psycopg2
sudo pip3 install python-etcd
sudo apt -y install patroni
sudo systemctl stop patroni
sudo systemctl disable patroni

```
### Настройка Patroni на всех узлах:
```bash

# Настройки записать в файл:
sudo nano /etc/patroni/config.yml
#---------------------------------------------------------------------------------
#---------------------# Узел 1:---------------------------------------------------
scope: pg_cluster
namespace: /service/
name: node1

restapi:
  listen: 10.10.8.25:8008
  connect_address: 10.10.8.25:8008

etcd3:
  hosts: 10.10.8.25:2379,10.10.8.26:2379,10.10.8.27:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true

  initdb:
    - auth: scram-sha-256
    - encoding: UTF8
    - data-checksums

  pg_hba:
    - host replication replicator 127.0.0.1/32 scram-sha-256
    - host replication replicator 10.10.8.25/0 scram-sha-256
    - host replication replicator 10.10.8.26/0 scram-sha-256
    - host replication replicator 10.10.8.27/0 scram-sha-256
    - host all all 0.0.0.0/0 scram-sha-256

  users:
    admin:
      password: admin
      options:
        - createrole
        - createdb

postgresql:
  listen: 10.10.8.25:5432
  connect_address: 10.10.8.25:5432
  data_dir: /var/lib/postgresql/17/main
  bin_dir: /usr/lib/postgresql/17/bin
  pgpass: /tmp/pgpass
  authentication:
    replication:
      username: replicator
      password: replicator
    superuser:
      username: postgres
      password: postgres
  parameters:
      # Directory for Unix socket
      unix_socket_directories: '.'
      # Password encryption method
      password_encryption: 'scram-sha-256'

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
#---------------------------------------------------------------------------------
#---------------------# Узел 2:---------------------------------------------------
scope: pg_cluster
namespace: /service/
name: node2

restapi:
  listen: 10.10.8.26:8008
  connect_address: 10.10.8.26:8008

etcd3:
  hosts: 10.10.8.25:2379,10.10.8.26:2379,10.10.8.27:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true

  initdb:
    - auth: scram-sha-256
    - encoding: UTF8
    - data-checksums

  pg_hba:
    - host replication replicator 127.0.0.1/32 scram-sha-256
    - host replication replicator 10.10.8.25/0 scram-sha-256
    - host replication replicator 10.10.8.26/0 scram-sha-256
    - host replication replicator 10.10.8.27/0 scram-sha-256
    - host all all 0.0.0.0/0 scram-sha-256

  users:
    admin:
      password: admin
      options:
        - createrole
        - createdb

postgresql:
  listen: 10.10.8.26:5432
  connect_address: 10.10.8.26:5432
  data_dir: /var/lib/postgresql/17/main
  bin_dir: /usr/lib/postgresql/17/bin
  pgpass: /tmp/pgpass
  authentication:
    replication:
      username: replicator
      password: replicator
    superuser:
      username: postgres
      password: postgres
  parameters:
      # Directory for Unix socket
      unix_socket_directories: '.'
      # Password encryption method
      password_encryption: 'scram-sha-256'

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
#---------------------------------------------------------------------------------
#---------------------# Узел 3:---------------------------------------------------
scope: pg_cluster
namespace: /service/
name: node3

restapi:
  listen: 10.10.8.27:8008
  connect_address: 10.10.8.27:8008

etcd3:
  hosts: 10.10.8.25:2379,10.10.8.26:2379,10.10.8.27:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true

  initdb:
    - auth: scram-sha-256
    - encoding: UTF8
    - data-checksums

  pg_hba:
    - host replication replicator 127.0.0.1/32 scram-sha-256
    - host replication replicator 10.10.8.25/0 scram-sha-256
    - host replication replicator 10.10.8.26/0 scram-sha-256
    - host replication replicator 10.10.8.27/0 scram-sha-256
    - host all all 0.0.0.0/0 scram-sha-256

  users:
    admin:
      password: admin
      options:
        - createrole
        - createdb

postgresql:
  listen: 10.10.8.27:5432
  connect_address: 10.10.8.27:5432
  data_dir: /var/lib/postgresql/17/main
  bin_dir: /usr/lib/postgresql/17/bin
  pgpass: /tmp/pgpass
  authentication:
    replication:
      username: replicator
      password: replicator
    superuser:
      username: postgres
      password: postgres
  parameters:
      # Directory for Unix socket
      unix_socket_directories: '.'
      # Password encryption method
      password_encryption: 'scram-sha-256'

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
#-----------------------------------------------------------------
#-----------------------------------------------------------------
# Внимательно смотреть на конфиг, не верно указал пути до postgresql
# т.к. в примере была старая версия, а у меня 17.!

# Отладка с помощью команды проверки Patroni 
sudo patroni --validate-config /etc/patroni/config.yml

# Запускаем Кластер Патрони:
sudo systemctl restart patroni
sudo systemctl enable patroni
sudo systemctl status patroni
sudo patronictl -c /etc/patroni/config.yml list

# Ошибка
# ... waiting for leader to bootstrap
# изменил имя scope: в фале /etc/patroni/config.yml
# так же есть решение с удаление файлов записи о кластере.

# Картинка с информацией о собранном кластере Patroni (Картинка ниже!):
``` 
![Кластера Patroni](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/cluster_patroni.jpg.jpg "Кластера Patroni")

```bash


# В случае успешного выполнения эта команда не выводит никаких данных, но она крайне важна, поскольку Patroni 
# управляет новым сервером PostgreSQL.

sudo systemctl disable --now postgresql

```
## -----------------------------------------------------
## Настраиваю PgBouncer на каждой ноде
```bash

# Устанавливаю пакеты на каждой ноде. 
sudo apt -y install pgbouncer
sudo systemctl stop pgbouncer
sudo systemctl disable pgbouncer

# Конфигурирование PgBouncer на каждой ноде
sudo cp -p /etc/pgbouncer/pgbouncer.ini /etc/pgbouncer/pgbouncer.ini.orig
sudo nano /etc/pgbouncer/pgbouncer.ini

# В разделе [databases] каждого экземпляра PgBouncer добавляем следующую строку:
#------------------------------------------------------------
# Узле 1
* = host=10.10.8.25 port=5432 dbname=postgres
#------------------------------------------------------------
# Узле 2
* = host=10.10.8.26 port=5432 dbname=postgres
#------------------------------------------------------------
# Узле 3
* = host=10.10.8.27 port=5432 dbname=postgres

# Найдити listen_addr в файле pgbouncer.ini и измените listen_addr= localhost на listen_addr= *

#  Обновить список пользователей в файле PgBouncer userlist.txt на каждом узле 
#------------------------------------------------------------
# Узле 1
sudo su postgres
psql -Atq -h 10.10.8.25 -p 5432 -U postgres -d postgres -c "SELECT concat('\"', usename, '\" \"', passwd, '\"') FROM pg_shadow" >> /etc/pgbouncer/userlist.txt
exit
#------------------------------------------------------------
# Узле 2
sudo su postgres
psql -Atq -h 10.10.8.26 -p 5432 -U postgres -d postgres -c "SELECT concat('\"', usename, '\" \"', passwd, '\"') FROM pg_shadow" >> /etc/pgbouncer/userlist.txt
exit
#------------------------------------------------------------
# Узле 3
sudo su postgres
psql -Atq -h 10.10.8.27 -p 5432 -U postgres -d postgres -c "SELECT concat('\"', usename, '\" \"', passwd, '\"') FROM pg_shadow" >> /etc/pgbouncer/userlist.txt
exit

# Перезапускаем службы и проверяем подключение на всех нодах:

sudo systemctl restart postgresql
sudo systemctl restart pgbouncer
sudo systemctl enable  pgbouncer

psql -h 10.10.8.25 -p 6432 -U postgres
psql -h 10.10.8.26 -p 6432 -U postgres
psql -h 10.10.8.27 -p 6432 -U postgres

# Картинка с информацией о работе PgBouncer (Картинка ниже!):
 ```
![PgBouncer](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/PgBouncer_test.jpg "PgBouncer")


## -----------------------------------------------------
## Установка и настройка HAProxy
```bash

sudo apt -y install haproxy
sudo systemctl stop haproxy
sudo systemctl disable haproxy

# Настройка HAProxy
sudo mv /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.orig
sudo nano /etc/haproxy/haproxy.cfg


#----------------------------------------------------------------
#--- Конфиг настройки--------------------------------------------
global
     log 127.0.0.1   local2
     log /dev/log    local0
     log /dev/log    local1 notice
     chroot /var/lib/haproxy
     stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
     stats timeout 30s
     user haproxy
     group haproxy
     maxconn 4000
     daemon

defaults
    mode                    tcp
    log                     global
    option                  tcplog
    retries                 3
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout check           10s
    maxconn                 3000

listen stats
    mode http
    bind *:7000
    stats enable
    stats uri /

listen primary
    bind 10.10.8.100:5000
    option httpchk OPTIONS /master
    http-check expect status 200
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server node1 10.10.8.25:6432 maxconn 100 check port 8008
    server node2 10.10.8.26:6432 maxconn 100 check port 8008
    server node3 10.10.8.27:6432 maxconn 100 check port 8008

listen standby
    bind 10.10.8.100:5001
    balance roundrobin
    option httpchk OPTIONS /replica
    http-check expect status 200
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    server node1 10.10.8.25:6432 maxconn 100 check port 8008
    server node2 10.10.8.26:6432 maxconn 100 check port 8008
    server node3 10.10.8.27:6432 maxconn 100 check port 8008
#----------------------------------------------------------------

sudo systemctl restart haproxy
sudo systemctl status haproxy

# Что-то не поднялся с первого раза
# Выполняю проверку: 
curl 10.10.8.25:8008/master -v
curl 10.10.8.26:8008/master -v
curl 10.10.8.27:8008/master -v
# Код 200 только для лидера возвращает.
# Остальные 503, судя но описанию, эти корректная работа.

# Чтобы развернуто посмотреть состояние HAProxy
# Нужно перейти по URL-адресу HAProxy и порту 7000 мы видим следующее для примера:
http://84.201.141.137:7000/
# публичный адрес у меня меняется.

# Картинка с информацией о работе HAProxy (Картинка ниже!):
``` 
![HAProxy](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/HAProxy_Info.jpg "HAProxy")

## -----------------------------------------------------
## Подключение и проверка работы в штатных условиях:
```bash

# Поднял дополнительный сервер в локальной сети
# Проверяю подключение:
psql -h 10.10.8.100 -p 5000 -U postgres

# Картинка с информацией о тесте подключения из локальной сети (Картинка ниже!):
``` 
![Подключения из локальной сети](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/local_test_connect.jpg "Подключения из локальной сети")

```bash
local_test_connect.jpg
# Начал проверять через pgAdmin удалось подключиться легко:
``` 
![Подключения из pgAdmin](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/pgAdmin_test_connect.jpg "Подключения из pgAdmin")

```bash
# а вот DBeaver, что-то мозги делает

# Погуглив некоторое время, я выяснил, что мне нужно всего лишь добавить эту простую конфигурацию в 
# pgbouncer.ini:
	
ignore_startup_parameters = extra_float_digits

# Исправил и дернул pgbouncer
# Картинка с информацией о тесте подключения через DBeaver (Картинка ниже!):
``` 
![Подключения через DBeaver](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/DBeaver_test_connect.jpg "Подключения через DBeaver")

```bash

Перезапускал сервера и внешний ip поменялись.!!!

# Для теста пробую создать базу на порту для записи.  
CREATE DATABASE test_db_otus;
# Картинка с информацией о созаднии базы (Картинка ниже!):
``` 
![Создание базы](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/Create_test_db.jpg "Создание базы")
```bash
# Видно, что создание прошло успешно.


# Далее пробую на порту для чтения выполнить запись
# Для заполнения данными:
# Картинка с информацией о попытки создания таблицы (Картинка ниже!):
``` 
![Создание таблицы порт 5001](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/Create_test_table.jpg "Создание таблицы порт 5001")
```bash
# А вот создать таблицу не получилось, мы обратились на 5001 порт, а он только для чтения.

# Еще раз пробуем создать только на через порт 5000
# Картинка с информацией о создании таблицы (Картинка ниже!):
``` 
![Создание таблицы порт 5000](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/Create_test_table_port_5000.jpg "Создание таблицы порт 5000")
```bash
# А вот сейчас таблица создалась успешно.
```

## -----------------------------------------------------
## Тестирование работы кластера после отключение нод:
```bash

# Проверяем на любой ноде кто лидер:
sudo patronictl -c /etc/patroni/config.yml list
  admin_s@pg-node-1:~$ sudo patronictl -c /etc/patroni/config.yml list
  + Cluster: pg_cluster1 (7546490960837068731) ----+-----------+
  | Member | Host       | Role    | State     | TL | Lag in MB |
  +--------+------------+---------+-----------+----+-----------+
  | node1  | 10.10.8.25 | Leader  | running   |  7 |           |
  | node2  | 10.10.8.26 | Replica | streaming |  7 |         0 |
  | node3  | 10.10.8.27 | Replica | streaming |  7 |         0 |
  +--------+------------+---------+-----------+----+-----------+

# Проверяю подключение на тестовом ПК:
admin_s@test-1:~$ psql -h 10.10.8.100 -p 5000 -U postgres
  Password for user postgres: 
  psql (17.6 (Ubuntu 17.6-1.pgdg24.04+1))
  Type "help" for help.

  postgres=#
admin_s@test-1:~$ psql -h 10.10.8.100 -p 5001 -U postgres
  Password for user postgres: 
  psql (17.6 (Ubuntu 17.6-1.pgdg24.04+1))
  Type "help" for help.

# без проблем все подключается
# порт 5000 запись
# порт 5001 чтение
# Показал для понимания что работает.

# так можно посмотреть локальный адрес 
SELECT inet_server_addr();
# Работает через DBeaver, а вот с локального тестового ПК через psql нет.

# Для теста создаю Базу, таблицу и заполняю данными:
# Создаем табличку:
CREATE TABLE student(
  id serial,
  fio char(100)
);

# Заполняем ее записями:
INSERT INTO student(fio) SELECT 'noname' FROM generate_series(1,1000000);
# Проверям есть ли данные:
SELECT * FROM student LIMIT 5;
  postgres=# SELECT * FROM student LIMIT 5;
  id |      fio
  ----+------------
    1 | noname
    2 | noname
    3 | noname
    4 | noname
    5 | noname
  (5 rows)

# Еще раз смотрим что сейчас показывает HAProxy:
# Картинка с информацией о работе HAProxy (Картинка ниже!):
``` 
![HAProxy](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/HAProxy_Info_Test.jpg "HAProxy")
```bash
# Видно, что мастер нода номер 1.

# Перезагружаем patroni на ноде лидер:
  sudo systemctl restart patroni

# Смотрим что сейчас показывает HAProxy:
# Картинка с информацией о работе HAProxy (Картинка ниже!):
``` 
![HAProxy](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/HAProxy_Info_Test1.jpg "HAProxy")
```bash
# Лидер поменялся на node3 

# Перезагружаем patroni на ноде нового лидера:
  sudo systemctl restart patroni

# Смотрим что сейчас показывает HAProxy:
# Картинка с информацией о работе HAProxy (Картинка ниже!):
``` 
![HAProxy](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/HAProxy_Info_Test2.jpg "HAProxy")
```bash
# Лидер поменялся на node2 
# Переключение происходит почти мгновенно!

# Проверим что данный не потеряны:
# Картинка с информацией о доступности информации из быза (Картинка ниже!):
``` 
![Тест после рестарта patroni](https://github.com/Z70007Z/Otus_PostgreSQL/blob/main/Tema_DP/picture/DBeaver_open_table.jpg "Тест после рестарта patroni")
```bash
# Вывод что все на месте.
```
## --------------------------------------------------------------
## Кластер Postgres на базе Patroni успешно собран и работает.!!!


# -----------------------------------------------------
# Материал для себя!
https://theamitdixit.wordpress.com/2024/02/17/achieving-high-availabilityha-in-postgresql-with-patroni-etcd-pgbouncer-and-haproxy/
https://habr.com/ru/companies/otus/articles/753294/
https://github.com/etcd-io/etcd/issues/11537
https://www.dmosk.ru/miniinstruktions.php?mini=patroni-consul-centos
https://sysad.su/установка-и-настройка-кластера-etcd-ubuntu-18/
https://www.percona.com/blog/haproxy-patroni-setup-using-health-check-endpoints-and-debugging/
https://edwin.baculsoft.com/2016/05/weird-exception-unsupported-startup-parameter-extra_float_digits-when-connecting-to-postgresql/
https://habr.com/ru/companies/slurm/articles/803739/



